{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder in PyTorch & Fastai\n",
    "\n",
    "Playing with the plain VAE and DFC-VAE as described in the following papers: \n",
    "https://arxiv.org/abs/1312.6114\n",
    "https://arxiv.org/abs/1610.00291\n",
    "\n",
    "Experimenting with TransposeConv vs Subpixel Conv upscaling methods.\n",
    "\n",
    "Using face-aligned & cropped MS Celeb face dataset of ~200k celebrity faces.\n",
    "\n",
    "Requires GPU device to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision\n",
    "\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.vision import *\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'sandro/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imlist = ImageList\\\n",
    "            .from_folder(DATA_PATH)\\\n",
    "            .split_by_rand_pct(valid_pct=0.01)\\\n",
    "            .label_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128 # batch size\n",
    "SZ = 64 # images will be resized to (SZxSZ) for training.\n",
    "\n",
    "db = imlist.transform(size=SZ, resize_method=ResizeMethod.SQUISH)\\\n",
    "            .databunch(bs=BS)\\\n",
    "            .normalize([(0.5, 0.5, 0.5),(0.5, 0.5, 0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining denormalizer for normalized images.\n",
    "\n",
    "class DeNormalize:\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, x, inplace=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        tensor = x if inplace else x.clone() \n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "denorm = DeNormalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing that data loader and denorm work\n",
    "\n",
    "x, _ = next(iter(db.train_dl))\n",
    "plt.imshow(denorm(x[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining conv and transposed conv layer blocks\n",
    "\n",
    "def get_conv(nf: int, of: int, ks: int, stride: int = 1, use_bn: bool = True):\n",
    "    \n",
    "    conv = nn.Conv2d(in_channels=nf, \n",
    "                  out_channels=of, \n",
    "                  kernel_size=ks, \n",
    "                  stride=stride, \n",
    "                  padding=ks // 2, \n",
    "                  bias=False)\n",
    "    \n",
    "    bn = nn.BatchNorm2d(of)\n",
    "    \n",
    "    act = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    return nn.Sequential(conv, bn, act) if use_bn else nn.Sequential(conv, act)\n",
    "\n",
    "def get_deconv(nf: int, of: int, ks: int, stride: int = 1, opad: int = 0, use_bn: bool = True):\n",
    "    \n",
    "    deconv = nn.ConvTranspose2d(in_channels=nf, \n",
    "                                out_channels=of, \n",
    "                                kernel_size=ks, \n",
    "                                stride=stride, \n",
    "                                padding=ks // 2,\n",
    "                                output_padding=opad,\n",
    "                                bias=False)\n",
    "    \n",
    "    bn = nn.BatchNorm2d(of)\n",
    "    \n",
    "    act = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    return nn.Sequential(deconv, bn, act) if use_bn else nn.Sequential(deconv, act)\n",
    "\n",
    "get_conv(3, 10, 5, 2, use_bn=False), get_deconv(10, 3, 5, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model and helper methods\n",
    "\n",
    "# copied from FastAI library.\n",
    "def icnr(x, scale=2, init=nn.init.kaiming_normal_):\n",
    "    \"ICNR init of `x`, with `scale` and `init` function.\"\n",
    "    ni,nf,h,w = x.shape\n",
    "    ni2 = int(ni/(scale**2))\n",
    "    k = init(torch.zeros([ni2,nf,h,w])).transpose(0, 1)\n",
    "    k = k.contiguous().view(ni2, nf, -1)\n",
    "    k = k.repeat(1, 1, scale**2)\n",
    "    k = k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
    "    x.data.copy_(k)\n",
    "\n",
    "class PixelShuffleICNR(nn.Module):\n",
    "    \"\"\"Pixel Shuffle with ICRN initialization.\"\"\"\n",
    "    \n",
    "    def __init__(self, ni: int, of: int = None, scale: int = 2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(ni, of * scale * scale, kernel_size=1)\n",
    "        #icnr(self.conv.weight)\n",
    "        \n",
    "        self.shuffle = nn.PixelShuffle(scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shuffle(self.conv(x))\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    \n",
    "    def __init__(self, *sizes):\n",
    "        super().__init__()\n",
    "        self.sizes = sizes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), *self.sizes)\n",
    "    \n",
    "def weights_init(m):\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        \n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        \n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def tranp_conv_decoder(nf: int, ks: int, z_dim: int):\n",
    "    \n",
    "    deconv1 = get_deconv(8*nf, 4*nf, ks, stride=2, opad=1)\n",
    "    deconv2 = get_deconv(4*nf, 2*nf, ks, stride=2, opad=1)\n",
    "    deconv3 = get_deconv(2*nf, nf, ks, stride=2, opad=1)\n",
    "    deconv4 = get_deconv(nf, 3, ks, use_bn=False)\n",
    "    \n",
    "    return nn.Sequential(nn.Linear(z_dim, 32768),\n",
    "                                Unflatten(512, 8, 8), \n",
    "                                deconv1, \n",
    "                                deconv2,\n",
    "                                deconv3,\n",
    "                                deconv4)\n",
    "\n",
    "def subpixel_conv_decoder(nf: int, ks: int, z_dim: int):\n",
    "    \n",
    "    return nn.Sequential(nn.Linear(z_dim, 32768),\n",
    "                                     Unflatten(512, 8, 8), \n",
    "                         \n",
    "                                     get_conv(8*nf, 4*nf, ks, use_bn=True),\n",
    "                                     PixelShuffleICNR(4*nf, 4*nf, scale=2),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                         \n",
    "                                     get_conv(4*nf, 2*nf, ks, use_bn=True),\n",
    "                                     PixelShuffleICNR(2*nf, 2*nf, scale=2),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                                     \n",
    "                                     get_conv(2*nf, nf, ks, use_bn=True), \n",
    "                                     PixelShuffle_ICNR(nf, nf, scale=2),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                         \n",
    "                                     get_conv(nf, 3, ks, use_bn=False)\n",
    "                                    )\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim: int, ks: int = 5, decoder=tranp_conv_decoder):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        nf = 64\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        conv1 = get_conv(3, nf, ks, use_bn=False)\n",
    "        conv2 = get_conv(nf, 2*nf, ks, stride=2)\n",
    "        conv3 = get_conv(2*nf, 4*nf, ks, stride=2)\n",
    "        conv4 = get_conv(4*nf, 8*nf, ks, stride=2)\n",
    "        \n",
    "        self.encoder = nn.Sequential(conv1, \n",
    "                                     conv2, \n",
    "                                     conv3, \n",
    "                                     conv4, \n",
    "                                     Flatten())\n",
    "        \n",
    "        self.mu = nn.Linear(32768, z_dim)\n",
    "        self.logvar = nn.Linear(32768, z_dim)\n",
    "        \n",
    "        self.decoder = decoder(nf, ks, z_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        conv = self.encoder(x)\n",
    "        \n",
    "        mu, logvar = self.mu(conv), self.logvar(conv)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def sample_z(self, mu, logvar):\n",
    "        \n",
    "        eps = torch.empty(mu.size(0), self.z_dim).normal_().to(mu.device)\n",
    "        return mu + eps * torch.exp(0.5 * logvar)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \n",
    "        return torch.tanh(self.decoder(z))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar = self.encode(x)\n",
    "        \n",
    "        z = self.sample_z(mu, logvar)\n",
    "        \n",
    "        return self.decode(z)\n",
    "\n",
    "model = VAE(z_dim=100)\n",
    "model.apply(weights_init)\n",
    "inp = torch.empty(5, 3, 64, 64).normal_()\n",
    "mu, logvar = model.encode(inp)\n",
    "out = model.decode(model.sample_z(mu, logvar))\n",
    "#assert inp.size() == out.size()\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining perceptual loss using pretrained VGG16 architecture.\n",
    "\n",
    "class Hook:\n",
    "\n",
    "    def __init__(self, m): \n",
    "        \n",
    "        self.feats = None\n",
    "        \n",
    "        self.hook = m.register_forward_hook(self._hook_fn)\n",
    "    \n",
    "    def _hook_fn(self, m, inp, out): \n",
    "        self.feats = out\n",
    "        \n",
    "    def remove(self): \n",
    "        self.hook.remove()        \n",
    "\n",
    "class PerceptualLoss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        vgg16 = models.vgg16_bn(pretrained=True)\n",
    "        \n",
    "        self.vgg16_head = nn.Sequential(*list(vgg16.children())[0])\n",
    "        \n",
    "        self.vgg16_head.to(DEVICE)\n",
    "        \n",
    "        # hard-coded layers used in perceptual loss\n",
    "        feat_indices = [2, 5, 9]\n",
    "        \n",
    "        for p in self.vgg16_head.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.hooks = [Hook(self.vgg16_head[i]) for i in feat_indices]\n",
    "        \n",
    "    def __call__(self, x_recon, x):\n",
    "        \n",
    "        self.vgg16_head(x_recon)\n",
    "        x_recon_feats = [h.feats.clone() for h in self.hooks]\n",
    "        self.vgg16_head(x)\n",
    "        loss = sum([F.mse_loss(x_recon_feats[i], h.feats, reduction='sum') for i, h in enumerate(self.hooks)])\n",
    "        return loss\n",
    "            \n",
    "    def close(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "            \n",
    "kacuri_loss = PerceptualLoss()\n",
    "kacuri_loss.vgg16_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE training loop. \n",
    "\n",
    "L = 1 # number of trials latent vector Z is sampled.\n",
    "N_EPOCHS = 10 # number of epochs\n",
    "LOG_INTERVAL = 50 # interval of logging info during training.\n",
    "ALPHA, BETA = 1, 0.75 # weights for KLD and Perceptual loss components.\n",
    "\n",
    "# defining a model\n",
    "model = VAE(z_dim=100, decoder=tranp_conv_decoder).to(DEVICE)\n",
    "\n",
    "lr = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "TB_LOG = False # whether to write info in tensorboard or not. \n",
    "if TB_LOG:\n",
    "    writer = SummaryWriter('~/tb-logs-1')\n",
    "\n",
    "# fetching sample from validation set to visualize reconstruction quality in the middle of training.\n",
    "x_val, _ = next(iter(db.valid_dl))\n",
    "\n",
    "model.train()\n",
    "step = 1\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = 0\n",
    "    for i_batch, (x, _) in enumerate(db.train_dl):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mu, logvar = model.encode(x)\n",
    "        \n",
    "        if TB_LOG:\n",
    "            writer.add_histogram('mu', mu, step)\n",
    "            writer.add_histogram('var', logvar.exp(), step)\n",
    "        \n",
    "        # monte carlo estimation of reconstruction loss.\n",
    "        recon_loss = 0\n",
    "        recon_loss_mse = 0\n",
    "        for l in range(L):\n",
    "            \n",
    "            z = model.sample_z(mu, logvar)\n",
    "        \n",
    "            x_recon = model.decode(z)\n",
    "        \n",
    "            recon_loss_mse += F.mse_loss(x_recon, x, reduction='sum')\n",
    "            recon_loss += kacuri_loss(x_recon, x)\n",
    "    \n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # final loss\n",
    "        loss = ALPHA * kld + BETA * ((recon_loss)/L)# + 0.05 * recon_loss_mse\n",
    "        \n",
    "        if TB_LOG:\n",
    "            writer.add_scalar('kld_loss', loss, step)\n",
    "            writer.add_scalar('recon_loss', loss, step)\n",
    "            writer.add_scalar('loss', loss, step)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                i_epoch, i_batch * x.size(0), len(db.train_ds),\n",
    "                100. * i_batch / len(db.train_dl),\n",
    "                loss.item() / x.size(0)))\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                    \n",
    "                x_val_recon = model(x_val[0:8])\n",
    "                    \n",
    "                #if TB_LOG:\n",
    "                    #for i in range(8):  \n",
    "                        #writer.add_image(f'Recon_x_{i}', denorm(x_val_recon[i]), step)\n",
    "                        \n",
    "                # visualizing reconstruction images\n",
    "                grd = make_grid(torch.cat([x_val[0:8], x_val_recon], dim=0), 8)\n",
    "                grd=denorm(grd)\n",
    "                plt.figure(figsize=(20, 20))\n",
    "                plt.imshow(grd.permute(1, 2, 0))\n",
    "                plt.show()\n",
    "        \n",
    "        step += 1\n",
    "            \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          i_epoch, train_loss / len(db.train_ds)))\n",
    "    \n",
    "    # learning rate decay\n",
    "    lr *= 0.75\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "if TB_LOG:        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "\n",
    "torch.save(model.state_dict(), f'model_mse_ep(5)_lr(2e-4)_bs({BS})_L(1)_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "\n",
    "model = VAE(z_dim=100, decoder=tranp_conv_decoder).to(DEVICE)\n",
    "with open(f'model_mse_ep(5)_lr(2e-4)_bs({BS})_L(1)_', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing reconstruction quality\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i_batch, (x, _) in enumerate(db.valid_dl):\n",
    "        mu, logvar = model.encode(x)\n",
    "        \n",
    "        z = model.sample_z(mu, logvar)\n",
    "        \n",
    "        x_recon = model.decode(z).cpu() \n",
    "        \n",
    "        x = x.cpu()\n",
    "        for i in range(x.size(0)):\n",
    "            img = torch.cat([denorm(x[i,:,:,:]).permute(1, 2, 0), torch.ones(x.size(-1), 1, 3), denorm(x_recon[i,:,:,:]).permute(1, 2, 0)], dim=1)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            if i >= 100:\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Visual Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading ms celeb annotation data\n",
    "df = pd.read_csv('list_attr_celeba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets vector of special facial feature (e.g. eyeglasses, beard, etc...)\n",
    "# generating P(z|x) for thousand images with given feature and averaging z's as described in original paper.\n",
    "\n",
    "def get_mean_vec(condition):\n",
    "\n",
    "    res = []\n",
    "    for i, xx in enumerate(df[condition].image_id.values):\n",
    "        try:\n",
    "            img = PIL.Image.open(os.path.join(DATA_PATH, xx))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "        targ_sz = resize_to(img, SZ, use_min=True)\n",
    "        img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')\n",
    "\n",
    "        x = torch.Tensor(np.array(img)).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "\n",
    "        x -= 0.5\n",
    "        x /= 0.5\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        mu, logvar = model.encode(x)\n",
    "        z = model.sample_z(mu, logvar)\n",
    "        res.append(z)\n",
    "        if i >= 1000: break\n",
    "\n",
    "\n",
    "    z = torch.mean(torch.cat(res, dim=0), dim=0).unsqueeze(0)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving smile vector\n",
    "z_eye = get_mean_vec((df.Eyeglasses == 1))\n",
    "z_no_eye = get_mean_vec((df.Eyeglasses == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualising eyeglass vector 1) reconstructed from mean eyeglass vector \n",
    "# 2) reconstructed from mean face no-eyeglass 3) difference betweeen 1)-3)\n",
    "\n",
    "plt.imshow(denorm(model.decode(z_eye)[0].cpu()).permute(1, 2, 0))\n",
    "plt.show()\n",
    "plt.imshow(denorm(model.decode(z_no_eye)[0].cpu()).permute(1, 2, 0))\n",
    "plt.show()\n",
    "plt.imshow(denorm(model.decode(z_eye - z_no_eye)[0].cpu()).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting eyeglassess onto selebs by adding eyeglass vector computed above.\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i_batch, (x, _) in enumerate(db.valid_dl):\n",
    "        mu, logvar = model.encode(x)\n",
    "        \n",
    "        z = model.sample_z(mu, logvar) + z_eye\n",
    "        \n",
    "        x_recon = model.decode(z).cpu() \n",
    "        \n",
    "        x = x.cpu()\n",
    "        for i in range(x.size(0)):\n",
    "            img = torch.cat([denorm(x[i,:,:,:]).permute(1, 2, 0), torch.ones(x.size(-1), 1, 3), denorm(x_recon[i,:,:,:]).permute(1, 2, 0)], dim=1)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            if i >= 100:\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing 2D space with basis vectors being \"Smile\" and \"Eyeglasses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean Z vector of no smile and with eyeglasses\n",
    "z_glass_smile = get_mean_vec((df.Eyeglasses == 1) & (df.Smiling == -1)).cpu()\n",
    "\n",
    "# mean Z vector of smile and with no eyeglasses\n",
    "z_glass_no_smile = get_mean_vec((df.Eyeglasses == -1) & (df.Smiling == 1)).cpu()\n",
    "\n",
    "# mean Z vector of no smile and with no eyeglasses\n",
    "z_no_glass_no_smile = get_mean_vec((df.Eyeglasses == -1) & (df.Smiling == -1)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the above vectors\n",
    "\n",
    "plt.imshow(denorm(model.decode(z_glass_smile.cuda())[0].cpu()).permute(1, 2, 0))\n",
    "plt.show()\n",
    "plt.imshow(denorm(model.decode(z_glass_no_smile.cuda())[0].cpu()).permute(1, 2, 0))\n",
    "plt.show()\n",
    "plt.imshow(denorm(model.decode(z_no_glass_no_smile.cuda())[0].cpu()).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing 2D space\n",
    "\n",
    "line = np.linspace(0, 1, 10)\n",
    "\n",
    "# definining 2D basis vectors\n",
    "I, J = z_glass_smile - z_no_glass_no_smile, z_glass_no_smile - z_no_glass_no_smile\n",
    "\n",
    "res = []\n",
    "for a in line:\n",
    "    for b in line:\n",
    "        z = z_no_glass_no_smile + a * I + b * J\n",
    "        x_recon = denorm(model.decode(z.to(DEVICE))).cpu()\n",
    "        res.append(x_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying space\n",
    "\n",
    "grd = make_grid(torch.cat(res, dim=0), 10)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(grd.permute(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
